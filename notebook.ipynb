{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# for testing\n",
    "from sklearn import naive_bayes, feature_extraction, feature_selection, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['this is the first document',\n",
    "          'this document is the second document',\n",
    "          'and this is the third one',\n",
    "          'is this the first document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self):\n",
    "        self.word_counter = None\n",
    "        self.index_to_word = None\n",
    "        self.word_to_index = None\n",
    "        self.document_frequency = None\n",
    "        self.n_docs = 0\n",
    "        self.n_vocab = 0\n",
    "\n",
    "    def _clean_data(self, data: list[str]) -> list[str]:\n",
    "        \"\"\"Cleans and preprocesses data by removing non-alphabet characters and lowering case.\"\"\"\n",
    "        return [' '.join(re.findall(r'[a-zA-Z]+', sentence)).lower() for sentence in data]\n",
    "\n",
    "    def _build_vocab(self, cleaned_data: list[str]) -> list[str]:\n",
    "        \"\"\"Builds vocabulary and mappings from cleaned data.\"\"\"\n",
    "        word_list = [word for sentence in cleaned_data for word in sentence.split()]\n",
    "        self.word_counter = dict(sorted(Counter(word_list).items()))\n",
    "        self.index_to_word = {idx: word for idx, word in enumerate(self.word_counter.keys())}\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(self.word_counter.keys())}\n",
    "        self.n_vocab = len(self.word_counter)\n",
    "\n",
    "    def _calculate_document_frequency(self, cleaned_data: list[str]):\n",
    "        \"\"\"Calculates document frequency for each word.\"\"\"\n",
    "        self.document_frequency = {word: 0 for word in self.word_to_index.keys()}\n",
    "        for sentence in cleaned_data:\n",
    "          splitted_sentence = sentence.split(\" \")\n",
    "          unique_word = np.unique(splitted_sentence)\n",
    "          for word in unique_word:\n",
    "            self.document_frequency[word] += 1\n",
    "\n",
    "    def _calculate_tfidf(self, sentence: str):\n",
    "        \"\"\"Calculates the TF-IDF vector for a single sentence.\"\"\"\n",
    "        counter = Counter(sentence.split())\n",
    "        row_sum = 0\n",
    "        tfidf_vector = np.zeros(self.n_vocab)\n",
    "\n",
    "        for j in range(self.n_vocab):\n",
    "            word = self.index_to_word[j]\n",
    "            # term frequency only caculated from freq on the sentence\n",
    "            # same as BoW (sklearn references)\n",
    "            tf = counter.get(word, 0)\n",
    "            df = self.document_frequency.get(word, 0)\n",
    "            # references: https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "            # adding 1 on n and df -> smoothing\n",
    "            idf = np.log((1 + self.n_docs) / (1 + df)) + 1\n",
    "            tf_idf = tf * idf\n",
    "\n",
    "            row_sum += np.square(tf_idf)\n",
    "            tfidf_vector[j] = tf_idf\n",
    "\n",
    "        l2_norm = np.sqrt(row_sum)\n",
    "        if l2_norm > 0:\n",
    "            tfidf_vector /= l2_norm\n",
    "\n",
    "        return tfidf_vector\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Fits the model on the data\"\"\"\n",
    "        cleaned_data = self._clean_data(data)\n",
    "        self.n_docs = len(cleaned_data)\n",
    "        self._build_vocab(cleaned_data)\n",
    "        self._calculate_document_frequency(cleaned_data)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Transforms new data based on the already fitted vocabulary and document frequencies.\"\"\"\n",
    "        if self.word_to_index is None or self.document_frequency is None:\n",
    "            raise ValueError(\"The TFIDF model must be fitted before calling transform.\")\n",
    "\n",
    "        cleaned_data = self._clean_data(data)\n",
    "        transformed_matrix = np.array([self._calculate_tfidf(sentence) for sentence in cleaned_data])\n",
    "        return transformed_matrix\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TFIDF()\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sklearn = feature_extraction.text.TfidfVectorizer()\n",
    "X_sklearn = tfidf_sklearn.fit_transform(corpus).toarray()\n",
    "X_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(X_tfidf, X_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = {}\n",
    "        self.feature_likelihoods = {}\n",
    "        self.join_log_likelihoods = []\n",
    "        self.alpha = 1.0  # additive laplacian smoothing\n",
    "\n",
    "        # for each class calculate prior and likelihoods\n",
    "        for cls in self.classes:\n",
    "            X_cls = X[y == cls]\n",
    "\n",
    "            # class prior = total this class / total dataset\n",
    "            self.class_priors[cls] = X_cls.shape[0] / X.shape[0]\n",
    "\n",
    "            # feature likelihoods = array with cols eq to total feature\n",
    "            # for each feature, sum all occurrence of that feature\n",
    "            # divide it by (sum features in that class + total feature)\n",
    "            self.feature_likelihoods[cls] = (np.sum(X_cls, axis=0) + (self.alpha)) / (\n",
    "                np.sum(X_cls) + (self.alpha * X.shape[1])\n",
    "            )\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        # for each data\n",
    "        for x in X:\n",
    "            posteriors = {}\n",
    "\n",
    "            # for each class\n",
    "            for cls in self.classes:\n",
    "                # normal\n",
    "                # posterior = (self.class_priors[cls])\n",
    "                # posterior *= np.prod(np.power(self.feature_likelihoods[cls], x))\n",
    "\n",
    "                # problem with normal is too small result, because fraction * fraction\n",
    "                # for example 0.3 * 0.5 * ......\n",
    "                # log version\n",
    "                log_posterior = np.log(self.class_priors[cls])\n",
    "\n",
    "                # math time\n",
    "                # log(A^B) = log(A) * B\n",
    "                # log(likelihoods ^ x ) = log(likelihoods) * x\n",
    "                # log(a*b) = log(a) + log(b), then:\n",
    "                # np.prod => np.sum\n",
    "                log_posterior += np.sum(np.log(self.feature_likelihoods[cls]) * x)\n",
    "\n",
    "                # add to class posterior\n",
    "                posteriors[cls] = np.float64(log_posterior)\n",
    "\n",
    "            # predict the class by choosing the highst log posterior\n",
    "            self.join_log_likelihoods.append(list(posteriors.values()))\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "            # same as np.argmax(list(posteriors.values()))\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "mnb.fit(X, y)\n",
    "print(mnb.predict(X[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-980.3129722 , -981.88514255, -911.99414188, -988.58679562,\n",
       "        -997.02412521, -995.70414588]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "pred = (clf.predict_joint_log_proba(X[2:3]))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-980.3129722 , -981.88514255, -911.99414188, -988.58679562,\n",
       "        -997.02412521, -995.70414588]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jll = np.array(mnb.join_log_likelihoods)\n",
    "jll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(jll, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelBinarizer:\n",
    "    def __init__(self):\n",
    "        self.rank_mapping = None\n",
    "\n",
    "    def fit(self, y):\n",
    "        unique_label = np.unique(y)\n",
    "        self.rank_mapping = {label: rank for rank, label in enumerate(unique_label)}\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        if len(self.rank_mapping) == 2:\n",
    "            result = np.zeros((len(y), 1))\n",
    "            for i in range(len(y)):\n",
    "                if y[i] == list(self.rank_mapping.keys())[0]:\n",
    "                    result[i] = 0\n",
    "                else:\n",
    "                    result[i] = 1\n",
    "            return result\n",
    "\n",
    "        result = np.zeros((len(y), len(self.rank_mapping)))\n",
    "        for i in range(len(y)):\n",
    "            if self.rank_mapping.get(y[i], None) is not None:\n",
    "                col = self.rank_mapping.get(y[i])\n",
    "                result[i][col] = 1\n",
    "        return result\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        self.fit(y)\n",
    "        return self.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1, 2, 6, 4, 2]\n",
    "assert np.array_equal(label_binarizer.fit_transform(y), lb.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['yes', 'no', 'no', 'yes']\n",
    "assert np.array_equal(label_binarizer.fit_transform(y), lb.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(X: np.ndarray, y: np.ndarray):\n",
    "    y = LabelBinarizer().fit_transform(y)\n",
    "\n",
    "    observed = np.dot(y.T, X)\n",
    "\n",
    "    class_prob = np.mean(y, axis=0).reshape(1, -1)\n",
    "    feature_count = np.sum(X, axis=0).reshape(1, -1)\n",
    "    expected = np.dot(class_prob.T, feature_count)\n",
    "\n",
    "    chi_square = np.sum((observed - expected) ** 2 / expected, axis=0)\n",
    "    return chi_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 3],\n",
    "              [0, 1, 5],\n",
    "              [5, 4, 1],\n",
    "              [6, 6, 2],\n",
    "              [1, 4, 0],\n",
    "              [0, 0, 0]])\n",
    "y = np.array([1, 1, 0, 0, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.38461538,  6.5       ,  8.90909091])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_square = chi_square(X, y)\n",
    "chi_square\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.38461538,  6.5       ,  8.90909091])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2_stats, _ = feature_selection.chi2(X, y)\n",
    "chi2_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(chi_square, chi2_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X: np.ndarray, y: np.ndarray, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split data into train, test\n",
    "    \"\"\"\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_size = int(X.shape[0] * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate accuracy score. \n",
    "    Formula: (TP + TN) / (TP + FP + TN + FN)\n",
    "    \"\"\"\n",
    "    return float(np.mean(y_true == y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metrics.accuracy_score(y_true, y_pred) == accuracy_score(np.array(y_true), np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(\n",
    "    model: MultinomialNB,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    k_folds: int = 5,\n",
    "    random_state: int = 42,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Cross validation for MultinomialNB model\n",
    "    \"\"\"\n",
    "    # shuffle data\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    X, y = X[indices], y[indices]\n",
    "\n",
    "    # split data into k folds\n",
    "    fold_size = len(y) // k_folds\n",
    "    scores = []\n",
    "\n",
    "    for i in range(k_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size if i != k_folds - 1 else len(y)\n",
    "\n",
    "        # per iteration, split data into train and validation\n",
    "        X_valid, y_valid = X[start:end], y[start:end]\n",
    "        X_train = np.concatenate([X[:start], X[end:]], axis=0)\n",
    "        y_train = np.concatenate([y[:start], y[end:]], axis=0)\n",
    "\n",
    "        # model to avoid data leakage between folds\n",
    "        model_clone = copy.deepcopy(model)\n",
    "\n",
    "        # tran and evaluate\n",
    "        model_clone.fit(X_train, y_train)\n",
    "        y_pred = model_clone.predict(X_valid)\n",
    "        score = accuracy_score(y_valid, y_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.randint(5, size=(10, 100))\n",
    "y = np.array([1, 2, 3, 4, 1, 2, 3, 4, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5, 0.0, 0.5, 0.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "k_folds = 5\n",
    "cv = cross_validation(mnb_model, X, y, k_folds=k_folds)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(cv) == k_folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
